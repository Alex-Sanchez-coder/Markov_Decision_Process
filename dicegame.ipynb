{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dice Game\n",
    "## Preamble\n",
    "\n",
    "Code bellow represents an agent that can play a simple dice game. Here are the basic rules:\n",
    "* You start with 0 points\n",
    "* Roll three fair six-sided dice\n",
    "* Now choose one of the following:\n",
    " * Stick, accept the values shown. If two or more dice show the same values, then all of them are flipped upside down: 1 becomes 6, 2 becomes 5, 3 becomes 4, and vice versa. The total is then added to your points and this is your final score.\n",
    " * OR reroll the dice. You may choose to hold any combination of the dice on the current value shown. Rerolling costs you 1 point – so during the game and perhaps even at the end your score may be negative. You then make this same choice again.\n",
    "\n",
    "The best possible score for this game is 18 and is achieved by rolling three 1s on the first roll.\n",
    "\n",
    "The reroll penalty prevents you from rolling forever to get this score. If the value of the current dice is greater than the expected value of rerolling them (accounting for the penalty), then you should stick.\n",
    "\n",
    "The optimal decision is independent of your current score. It does not matter whether it is your first roll with a current score of 0, or your twentieth roll with a current score of -19 (in which case a positive end score is impossible), in either of these cases if you roll three 6s (which, if you stick, will only add 3 points) then you still expect to get a *better* end score by rerolling and taking the penalty. Almost any other roll will beat it, so it's still the right choice to maximise your score.\n",
    "\n",
    "It is pretty obvious that you should stick on three 1s, and reroll on three 6s. Should you hold any of the 6s when you reroll? What about other values? What should you do if the dice come up 3, 4, 5?\n",
    "\n",
    "We do not know what numbers will come up when we roll, but we do know exactly what the probability of any given roll is. This is the point of the probabilistic reasoning section of the unit; if we can model the true probabilities then we can mathematically calculate the optimal policy. Not all real world situations use dice, but these techniques work well even if we can only estimate the true probabilities.\n",
    "\n",
    "### Play The Game\n",
    "You can play the game in the following cell. Change the `SKIP_GAME` constant to `False` to enable this cell. \n",
    "<br> *Make sure to change it back to `True` before submitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_GAME = True\n",
    "if not SKIP_GAME:\n",
    "    %run dice_game.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code supports playing this game with many possible modifications – you can change the number of dice, the values on the dice, or even make biased (weighted) dice that are more likely to roll certain values. More on this later.\n",
    " \n",
    "### Choice of Algorithm\n",
    "*value iteration* technique will work well here to produce a strategy for the game, which the agent can then follow. Working with the parameters that will be suit the value iteration algorithm to maximise your score is an option.\n",
    "\n",
    "However, there are many other possible options. Simply calculating the expected value of a single roll will produce a much stronger strategy than playing randomly. You could also look up various other approaches that can be applied to Markov decision processes, such as policy iteration.\n",
    "\n",
    "A great agent will require a particularly efficient implementation value iteration with intelligent choice of parameters.\n",
    "\n",
    "\n",
    "## Dice Game Class\n",
    "A class called `DiceGame` is provided within `dice_game.py` \n",
    "When a DiceGame object is created, by default you will get the rules as stated above. \n",
    "```python\n",
    "game = DiceGame()\n",
    "```\n",
    "Creates a game with 3 normal 6-sided dice. \n",
    "\n",
    "Game mechanics can be modified, and can be achieved by using the other constructor arguments, for example:\n",
    "```python\n",
    "game = DiceGame(dice=4, sides=3, values=[1, 2, 6], bias=[0.1, 0.1, 0.8], penalty=2)\n",
    "```\n",
    "will create a game where you roll 4 dice, each with 3 sides, labelled 1, 2, and 6, where each die is far more likely to roll a 6 than they are to roll a 1 or a 2, and furthermore the penalty for rerolling is now 2 points instead of 1. *Note: this does not necessarily result in an interesting game.*\n",
    "\n",
    "In games with unusual values or sides (3-sided dice are unusual without trying to turn them upside down), when there are duplicates, `value[i]` becomes `value[-i]`. With odd-sided dice, the middle value will flip onto itself.\n",
    "\n",
    "Once created, the `DiceGame` object can be run in two different modes, *simulation* and *analysis*. It is likely that you will mostly use [*analysis* mode](#Analysis-Mode) to derive your agent's behaviour, but either way some understanding of simulation mode might be useful.\n",
    "\n",
    "### Simulation Mode\n",
    "The object provides the methods required to simulate playing the game. This might be useful for trsting purposes. The current dice values are found by calling `get_dice_state()`, they will always be listed in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dice_game import DiceGame\n",
    "import numpy as np\n",
    "\n",
    "# setting a seed for the random number generator gives repeatable results, making testing easier!\n",
    "np.random.seed(111)\n",
    "\n",
    "game = DiceGame()\n",
    "game.get_dice_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To roll the dice, you call the `roll` method which takes one parameter: a tuple representing which dice you want to hold, numbered from zero. We rolled (2, 3, 4). Suppose we want to hold the 2, we would pass the tuple `(0,)` into the `roll` method (note we need to include the comma so that Python knows this is a tuple).\n",
    "\n",
    "The `roll` method returns a tuple containing: the reward for this action, the new state, and whether the game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "(2, 2, 5)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "reward, new_state, game_over = game.roll((0,))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we are happy and wish to stick to get our final score. We can call the `roll` method and supply a tuple containing all three dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(5, 5, 5)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reward, new_state, game_over = game.roll((0, 1, 2))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the return value is just the reward for the action, in this case 15. To get our final score we can inspect `game.score`. We rerolled once, so expect to get a score of 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(game.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to play again we can call `game.reset()` which returns the new starting dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Mode\n",
    "In analysis mode, you are not playing the game, but asking the object for *all possible outcomes of certain actions*.\n",
    "\n",
    "First of all, it is useful to know that all the possible states and all the possible actions are stored inside the object. Try changing the game mechanics on the first line (e.g. add `dice=2` to the constructor) and see how the other information is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 of 56 possible dice rolls are: [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4), (1, 1, 5)]\n",
      "The possible actions on any given turn are: [(), (0,), (1,), (2,), (0, 1), (0, 2), (1, 2), (0, 1, 2)]\n"
     ]
    }
   ],
   "source": [
    "game = DiceGame()\n",
    "print(f\"The first 5 of {len(game.states)} possible dice rolls are: {game.states[0:5]}\")\n",
    "print(f\"The possible actions on any given turn are: {game.actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the most important method is `get_next_states(action, dice_state)`. This allows you to get all the possible resulting states for any given state and action.\n",
    "\n",
    "Earlier we had the roll of `(2, 3, 4)` and decided to hold the 2. The game can calculate all possible outcomes for us, and crucially will also give us the probability of each state occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would get roll of (1, 1, 2) with probability 0.02777777777777778\n",
      "Would get roll of (1, 2, 2) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 3) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 4) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 5) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 2, 2) with probability 0.02777777777777778\n",
      "Would get roll of (2, 2, 3) with probability 0.055555555555555566\n",
      "Would get roll of (2, 2, 4) with probability 0.055555555555555566\n",
      "Would get roll of (2, 2, 5) with probability 0.055555555555555566\n",
      "Would get roll of (2, 2, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 3, 3) with probability 0.02777777777777778\n",
      "Would get roll of (2, 3, 4) with probability 0.055555555555555566\n",
      "Would get roll of (2, 3, 5) with probability 0.055555555555555566\n",
      "Would get roll of (2, 3, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 4, 4) with probability 0.02777777777777778\n",
      "Would get roll of (2, 4, 5) with probability 0.055555555555555566\n",
      "Would get roll of (2, 4, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 5, 5) with probability 0.02777777777777778\n",
      "Would get roll of (2, 5, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 6, 6) with probability 0.027777777777777804\n"
     ]
    }
   ],
   "source": [
    "game = DiceGame()\n",
    "states, game_over, reward, probabilities = game.get_next_states((0,), (2, 3, 4))\n",
    "for state, probability in zip(states, probabilities):\n",
    "    print(f\"Would get roll of {state} with probability {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method also works consistently when all dice are held, reporting that this action would cause the game to be over and giving the reward. Note that the list of states returned contains the value `None`. This is to denote that the game has entered a terminal state – no further actions would be allowed. The game does not return the final dice here, because that is another valid state (from which there would still be actions available). Also, the `reward` value is not the same as the final `score` of any given game, because it does not include any possible previous penalties for rerolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "True\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "states, game_over, reward, probabilities = game.get_next_states((0, 1, 2), (2, 2, 5))\n",
    "print(states)\n",
    "print(game_over)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "\n",
    "Let's start with some example agents, so you can see the format we will use. In the cell below are two agents which do not play particularly well. One always holds immediately, the other will keep re-rolling all dice until they get the best possible dice (`(1, 1, 1)` or `(1, 1, 6)`), ignoring the massive penalty this will incur from re-rolling. Neither of them is considering the probabilities involved in the game.\n",
    "\n",
    "There is also a function which will run the game with an instance of a given agent. When you run the cell, it will simulate a game with each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all the variables from previous cells\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent: \n",
      "\tAlwaysHoldAgent\n",
      "Starting dice: \n",
      "\t(1, 1, 6)\n",
      "\n",
      "Action 1: \t(0, 1, 2)\n",
      "\n",
      "Final dice: (6, 6, 6), score: 18\n",
      "\n",
      "\n",
      "Testing agent: \n",
      "\tPerfectionistAgent\n",
      "Starting dice: \n",
      "\t(5, 5, 6)\n",
      "\n",
      "Action 1: \t()\n",
      "Dice: \t\t(4, 5, 6)\n",
      "Action 2: \t()\n",
      "Dice: \t\t(1, 3, 3)\n",
      "Action 3: \t()\n",
      "Dice: \t\t(1, 5, 5)\n",
      "Action 4: \t()\n",
      "Dice: \t\t(2, 4, 6)\n",
      "Action 5: \t()\n",
      "Dice: \t\t(1, 3, 4)\n",
      "Action 6: \t()\n",
      "Dice: \t\t(2, 3, 5)\n",
      "Action 7: \t()\n",
      "Dice: \t\t(2, 3, 6)\n",
      "Action 8: \t()\n",
      "Dice: \t\t(1, 5, 6)\n",
      "Action 9: \t()\n",
      "Dice: \t\t(5, 6, 6)\n",
      "Action 10: \t()\n",
      "Dice: \t\t(3, 3, 6)\n",
      "Action 11: \t()\n",
      "Dice: \t\t(1, 1, 2)\n",
      "Action 12: \t()\n",
      "Dice: \t\t(1, 1, 1)\n",
      "Action 13: \t(0, 1, 2)\n",
      "\n",
      "Final dice: (6, 6, 6), score: 6\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dice_game import DiceGame\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DiceGameAgent(ABC):\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "    \n",
    "    @abstractmethod\n",
    "    def play(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AlwaysHoldAgent(DiceGameAgent):\n",
    "    def play(self, state):\n",
    "        return (0, 1, 2)\n",
    "\n",
    "\n",
    "class PerfectionistAgent(DiceGameAgent):\n",
    "    def play(self, state):\n",
    "        if state == (1, 1, 1) or state == (1, 1, 6):\n",
    "            return (0, 1, 2)\n",
    "        else:\n",
    "            return ()\n",
    "        \n",
    "        \n",
    "def play_game_with_agent(agent, game, verbose=False):\n",
    "    state = game.reset()\n",
    "    \n",
    "    if(verbose): print(f\"Testing agent: \\n\\t{type(agent).__name__}\")\n",
    "    if(verbose): print(f\"Starting dice: \\n\\t{state}\\n\")\n",
    "    \n",
    "    game_over = False\n",
    "    actions = 0\n",
    "    while not game_over:\n",
    "        action = agent.play(state)\n",
    "        actions += 1\n",
    "        \n",
    "        if(verbose): print(f\"Action {actions}: \\t{action}\")\n",
    "        _, state, game_over = game.roll(action)\n",
    "        if(verbose and not game_over): print(f\"Dice: \\t\\t{state}\")\n",
    "\n",
    "    if(verbose): print(f\"\\nFinal dice: {state}, score: {game.score}\")\n",
    "        \n",
    "    return game.score\n",
    "\n",
    "\n",
    "def main():\n",
    "    # random seed makes the results deterministic\n",
    "    # change the number to see different results\n",
    "    # or delete the line to make it change each time it is run\n",
    "    #np.random.seed(1)\n",
    "    \n",
    "    game = DiceGame()\n",
    "    \n",
    "    agent1 = AlwaysHoldAgent(game)\n",
    "    play_game_with_agent(agent1, game, verbose=True)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    agent2 = PerfectionistAgent(game)\n",
    "    play_game_with_agent(agent2, game, verbose=True)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c23c3b5f5b787d247eba0e6f24fcf348",
     "grade": false,
     "grade_id": "cell-cb208d0e0467d373",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from dice_game import *\n",
    "import time\n",
    "\n",
    "class MyAgent(DiceGameAgent):\n",
    "    \n",
    "    #__slots__ = 'game' # Reduce the size of object\n",
    "    \n",
    "    #####################################################\n",
    "    ################   Constructor  #####################\n",
    "    #####################################################\n",
    "    # Value iteration algorithm inside\n",
    "    \n",
    "    def __init__(self, game): \n",
    "    \n",
    "        super().__init__(game)\n",
    "        #print(sys.getsizeof(game))\n",
    "        \n",
    "        # Paramaters and varaibles:\n",
    "        # self.gamma = discount factor-> Optimal bteween 0.90 an 1 \"Modify if it´s needed\"\n",
    "        # converged-> Flag to the \"while loop-Value_iteration algorithm\" for self.convergence_criterion \"Do NOT Modify\"\n",
    "        # self.convergence_criterion-> Defining a treshold to limit the value_iteration function \"Modify if it´s needed\"\n",
    "        #                              Estimated average runs: 6 (2.3s ->1e-9), (1.4s->1e-3), (1.6s->1e-4)\n",
    "        # self.memory_backup-> Save states and actions that agent has already explored \n",
    "        # self.policy-> Save all States with their action\n",
    "        # self.values_s-> Save all states eith their values\n",
    "        \n",
    "        converged = False\n",
    "        convergence_criterion = 1e-3\n",
    "        self.gamma = 0.95\n",
    "        self.memory_backup = {}\n",
    "        self.policy = {}\n",
    "        self.values_s = {}\n",
    "        \n",
    "        # Getting all states and values based on \"game\" object (baseline dice=3, sides=6)\n",
    "        \n",
    "        for state in game.states:\n",
    "            self.values_s[state] = game.final_score(state)\n",
    "        \n",
    "        #####################################################\n",
    "        ##########   Value_Iteration Algorithm  #############\n",
    "        #####################################################\n",
    "        \n",
    "        while not converged:\n",
    "            \n",
    "            convergence_factor = 0 # Initialazing the convergence_factor for each iteration\n",
    "            \n",
    "            for state in game.states: # self.values_s.keys() could be used but time does NOT decrease\n",
    "                \n",
    "                new_value = float(\"-inf\") # Initialazing the value with the highest negative number for each state\n",
    "                old_value = self.values_s[state] # Save the state´s value\n",
    "                                \n",
    "                for action in game.actions:\n",
    "                    \n",
    "                    Bellman_value = self.Bellman_value_function(action, state)\n",
    "\n",
    "                    if Bellman_value > new_value: # Updating the Bellman´s value (maximazing value) \n",
    "                        new_value = Bellman_value \n",
    "                        \n",
    "                        optimal_action = action # Extracting the induced policy / action to keep\n",
    "                        #print(f\"optimal_action->{optimal_action}\")\n",
    "                \n",
    "                self.values_s[state] = new_value # Assigning the updated value to the corresponding state\n",
    "                self.policy[state] = optimal_action # Assigning the induced policy according to the best last value\n",
    "                \n",
    "            # convergence method based on self.convergence_criterion\n",
    "                convergence_factor = max(convergence_factor, abs(old_value - new_value))\n",
    "            if convergence_factor < convergence_criterion:\n",
    "                break\n",
    "                \n",
    "        return None\n",
    "            \n",
    "    \n",
    "    #####################################################\n",
    "    ##########   Bellman_value_function   ###############\n",
    "    #####################################################\n",
    "    # Getting the Bellman´s value\n",
    "    \n",
    "    def Bellman_value_function(self, action, state, V_s = 0):\n",
    "        \n",
    "        # Function to reduce process time\n",
    "        self.states_prob, self.game_over, self.reward = self.memory_backup_function(action, state) \n",
    "        \n",
    "        for next_state, probability in self.states_prob:\n",
    "            \n",
    "            if not self.game_over: # Iterating until True\n",
    "                V_s += probability * (self.reward + self.gamma * self.values_s.get(next_state))\n",
    "            else: # End game (end_game = True)\n",
    "                V_s += probability * (self.reward + self.gamma * self.game.final_score(state))\n",
    "    \n",
    "        return V_s\n",
    "    \n",
    "    \n",
    "    #####################################################\n",
    "    ##########   memory_backup_function   ###############\n",
    "    #####################################################\n",
    "    # Save actions and sates (based on get_next_states) to self.memory_backup in order to speed up\n",
    "    # the calculation and search process (- time but + memory)\n",
    "    # Return a dictionary with states and their probabilities (self.states_prob)\n",
    "    # Return the status of game over (flag) and the reward\n",
    "\n",
    "    def memory_backup_function(self, action, state):\n",
    "        \n",
    "        if (action, state) not in self.memory_backup:\n",
    "            self.memory_backup[(action, state)] = self.game.get_next_states(action, state)\n",
    "   \n",
    "        self.other_states, self.game_over, self.reward, self.probabilities = self.memory_backup.get((action, state))\n",
    "        \n",
    "        self.states_prob = zip(self.other_states, self.probabilities)\n",
    "        \n",
    "        return self.states_prob, self.game_over, self.reward\n",
    "    \n",
    "    \n",
    "    #####################################################\n",
    "    ###################   play   ########################\n",
    "    #####################################################\n",
    "    # Return the action based on the state according to updated best last value in Bellman´s equation\n",
    "    \n",
    "    def play(self, state):\n",
    "        action = self.policy.get(state)\n",
    "        return action\n",
    "\n",
    "#def main():\n",
    "    \n",
    "    #game = DiceGame()\n",
    "    \n",
    "    #My_agent = MyAgent(game)\n",
    "    #play_game_with_agent(My_agent, game, verbose=True)\n",
    "    \n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #total_time = 0\n",
    "    #start_time = time.process_time()\n",
    "    #main()\n",
    "    #total_time += time.process_time() - start_time\n",
    "\n",
    "    #print(f\"Total time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Testing Details\n",
    "Agent will be tested in two ways. First it will be tested in actual random games, and the average score will be measured. All students will get the exact same dice rolls, so the best strategies will get the most points. Second, it will be analysed in specific circumstances (i.e. specific dice rolls) to test what it does compared to the optimal strategy.\n",
    "\n",
    "In addition, the tests will be repeated with *extended rules*, i.e. not using the default game with three 6-sided fair dice. You can read more about how this affects grading in [this section](#Choice-of-Algorithm). \n",
    "\n",
    "On the testing machine, the agent must take less than 30 seconds to construct, and less than 2 seconds to produce each action.\n",
    "\n",
    "The best way to improve the performance is through a detailed understanding and smart choice of AI algorithms. This implementaion is ***not*** meant to test the ability to write multi-threaded code or any other kind of high-performance code optimisations. \n",
    "\n",
    "#### Test Cell\n",
    "Run the following cell to test the agent 10 times and print the average score.\n",
    "\n",
    "To enable the tests, set the constant `SKIP_TESTS` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SKIP_TESTS = True\n",
    "\n",
    "def tests():\n",
    "    import time\n",
    "\n",
    "    total_score = 0\n",
    "    total_time = 0\n",
    "    n = 10\n",
    "\n",
    "    np.random.seed()\n",
    "\n",
    "    print(\"Testing basic rules.\")\n",
    "    print()\n",
    "\n",
    "    game = DiceGame()\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    test_agent = MyAgent(game)\n",
    "    total_time += time.process_time() - start_time\n",
    "\n",
    "    for i in range(n):\n",
    "        start_time = time.process_time()\n",
    "        score = play_game_with_agent(test_agent, game)\n",
    "        total_time += time.process_time() - start_time\n",
    "\n",
    "        print(f\"Game {i} score: {score}\")\n",
    "        total_score += score\n",
    "\n",
    "    print()\n",
    "    print(f\"Average score: {total_score/n}\")\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")\n",
    "    \n",
    "if not SKIP_TESTS:\n",
    "    tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opt-In For Extended Rules\n",
    "Now, the agent will be tested with rules other than the defaults.\n",
    "Set the constant `TEST_EXTENDED_RULES` to `True` on the next line. Another test will be performed to check if the code still works. If an error occuts, it would indicate that the code is not supporting the extended rules properly.\n",
    "\n",
    "Refer to [this section](#Choice-of-Algorithm) to understand more about how extended rules factor into your possible grade.\n",
    "\n",
    "**Note:** you need to have `SKIP_TESTS` set to `False` in the cell above (and run it!) to enable the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EXTENDED_RULES = True\n",
    "\n",
    "def extended_tests():\n",
    "    total_score = 0\n",
    "    total_time = 0\n",
    "    n = 10\n",
    "\n",
    "    print(\"Testing extended rules – two three-sided dice.\")\n",
    "    print()\n",
    "\n",
    "    game = DiceGame(dice=2, sides=3)\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    test_agent = MyAgent(game)\n",
    "    total_time += time.process_time() - start_time\n",
    "\n",
    "    for i in range(n):\n",
    "        start_time = time.process_time()\n",
    "        score = play_game_with_agent(test_agent, game)\n",
    "        total_time += time.process_time() - start_time\n",
    "\n",
    "        print(f\"Game {i} score: {score}\")\n",
    "        total_score += score\n",
    "\n",
    "    print()\n",
    "    print(f\"Average score: {total_score/n}\")\n",
    "    print(f\"Average time: {total_time/n:.5f} seconds\")\n",
    "\n",
    "if not SKIP_TESTS and TEST_EXTENDED_RULES:\n",
    "    extended_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking code\n",
    "The following cell tests if your notebook is ready\n",
    "\n",
    "Restart the kernel and run the entire notebook (Kernel → Restart & Run All). Now look at the output of the cell below. \n",
    "\n",
    "*If there is no output, then it is not ready.* Either your code is still running (did you forget to skip tests?) or it caused an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c56e3d10de8280302b2e1d6ee5136b4b",
     "grade": false,
     "grade_id": "cell-0e662447caaa3bcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: TEST_EXTENDED_RULES is ON\n",
      "\n",
      "All checks passed. When you are ready to submit, upload the notebook and readme file to the\n",
      "assignment page, without changing any filenames.\n",
      "\n",
      "If you need to submit multiple files, you can archive them in a .zip file. (No other format.)\n"
     ]
    }
   ],
   "source": [
    "def submission_tests():\n",
    "    import sys\n",
    "    import pathlib\n",
    "\n",
    "    fail = False;\n",
    "\n",
    "    if not SKIP_TESTS:\n",
    "        fail = True;\n",
    "        print(\"You must set the SKIP_TESTS constant to True in the earlier cell.\")\n",
    "\n",
    "    p1 = pathlib.Path('./readme.txt')\n",
    "    p2 = pathlib.Path('./readme.md')\n",
    "    if not (p1.is_file() or p2.is_file()):\n",
    "        fail = True;\n",
    "        print(\"You must include a separate file called readme.txt or readme.md in your submission.\")\n",
    "\n",
    "    p3 = pathlib.Path('./dicegame.ipynb')\n",
    "    if not p3.is_file():\n",
    "        fail = True\n",
    "        print(\"This notebook file must be named dicegame.ipynb\")\n",
    "\n",
    "    if \"MyAgent\" not in globals():\n",
    "        fail = True;\n",
    "        print(\"You must include a class called MyAgent as defined above.\")\n",
    "    else:    \n",
    "        game = DiceGame()\n",
    "        agent = MyAgent(game)\n",
    "        action = agent.play((1, 1, 1))\n",
    "\n",
    "        if action not in game.actions:\n",
    "            print(\"Warning:\")\n",
    "            print(\"Your agent does not seem to produce a valid action with the default rules.\")\n",
    "            print()\n",
    "            print(\"Your assignment is unlikely to get any marks from the autograder. While we will\")\n",
    "            print(\"try to check it manually to assign some partial credit, we encourage you to ask\")\n",
    "            print(\"for help on the forum or directly to a tutor.\")\n",
    "            print()\n",
    "            print(\"Please use the readme file to explain your code anyway.\")\n",
    "\n",
    "        if TEST_EXTENDED_RULES:\n",
    "            print(\"Info: TEST_EXTENDED_RULES is ON\")\n",
    "            game = DiceGame(dice=2, sides=8)\n",
    "            agent = MyAgent(game)\n",
    "            try:\n",
    "                action = agent.play((7, 8))\n",
    "            except:\n",
    "                action = None\n",
    "\n",
    "            if action not in game.actions:\n",
    "                fail = True\n",
    "                print(\"Your agent does not produce a valid action with the extended rules.\")\n",
    "                print(\"Turn off TEST_EXTENDED_RULES if you cannot fix this error.\")\n",
    "        else:\n",
    "            print(\"Info: TEST_EXTENDED_RULES is OFF (extended rules will not be tested)\")\n",
    "\n",
    "    if fail:\n",
    "        print()\n",
    "        sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "    else:\n",
    "        print()\n",
    "        print(\"All checks passed. When you are ready to submit, upload the notebook and readme file to the\")\n",
    "        print(\"assignment page, without changing any filenames.\")\n",
    "        print()\n",
    "        print(\"If you need to submit multiple files, you can archive them in a .zip file. (No other format.)\")\n",
    "        \n",
    "submission_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e79088eef80312d3d223f6a463e5648",
     "grade": true,
     "grade_id": "cell-7ffe19fac82d0fb1",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. Do not delete or change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
